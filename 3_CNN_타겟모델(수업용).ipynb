{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"collapsed_sections":["SFvYLebFZzGG","5uzeJaMB7DPa","py0V6UwC6_kH"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NcAHF4g8Xa93"},"source":["# Convolutional Networks 타겟모델"]},{"cell_type":"code","metadata":{"id":"bJG0K_T7wifE"},"source":["#@title Imports (RUN ME!) { display-mode: \"form\" }\n","\n","!pip install tensorflow-gpu==2.0.0-beta0 > /dev/null 2>&1\n","!pip -q install pydot_ng > /dev/null 2>&1\n","!pip -q install graphviz > /dev/null 2>&1\n","!apt install graphviz > /dev/null 2>&1\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython import display\n","%matplotlib inline\n","\n","print(\"TensorFlow executing eagerly: {}\".format(tf.executing_eagerly()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u5_zoqk-YK0D"},"source":["## Convolutional Architectures (컨볼루션 구조)\n","\n","컨볼루션 구조는 기존 뉴럴네트워크(Neural network)에 비해 convolution layer와 max pooling을 이용해서 공간 정보를 유지하면서 다음 층에 전달이 가능하다. 또한, 뉴럴네트워크에 비해 convolution filter는 상대적으로 뉴럴네트워크의 파라미터의 수보다 작기에 파라미터의 수를 줄일 수 있는 장점이 있다. 이미지 처리에서 특히 좋은 성능을 제공하고 있으며, 자연어처리 등에서도 활용된다. \n","\n","적대적 샘플 생성시 타겟이 되는 대표적인 모델인 컨볼루션 뉴럴 네트워크 모델에 기본 구조를 살펴보고자 한다."]},{"cell_type":"markdown","metadata":{"id":"vOp67cfpaTlC"},"source":["### [1] Convolutional Layers (컨볼루션 층)"]},{"cell_type":"markdown","metadata":{"id":"6JbXSVKulDoD"},"source":["2차원 (2-dimensional convolutional layers)는 입력 데이터(통상 3D input tensor, 예를 들어, [가로, 세로, 채널수])을 받고 결과값으로 convional filter의 계산을 걸쳐 output을 출력함. 이때, output를 feature map 이라고 통상이라고 부른다. 각 convolution filter(또는 그냥 filter라고 함)는 컨볼루션 연산을 실시하며, stride와 padding의 영향에 따라 연산과정이 있다.\n","\n","\n","수업간 이 부분에 대해서 예시로 계산해서 이해할 필요가 있다.\n","\n","\n","이렇게 계산된 feature map은 활성화함수(activation function)을 거치게 되는데, CNN 연산에서는 보통 ReLU 함수를 사용한다.\n","ReLU 함수의 식($f(\\cdot)$)은 아래와 같다. \n","\n","\n","$f(x) = max\\{0, x\\}$\n","\n","\n","연산 과정을 이미지로 표현하면 아래와 같다. \n","\n","\n","아래 그림에서, filter의 사이즈는 [4,4,3]으로 가로 4, 세로 4, 채널 3을 의미한다. stride는 1이고, padding은 1이다.\n","\n","![Convolution Animation](https://i.stack.imgur.com/FjvuN.gif)\n"]},{"cell_type":"markdown","metadata":{"id":"eLiuT6TcmXw8"},"source":["컨볼루션 층에서 하이퍼파라미터는 아래와 같다. \n","* **Filters**(필터) defines the number of filters in the layer\n","* **Kernel Size**(커널 사이즈) defines the width and height of the filters (also called \"kernels\") in the layer. Note that kernels always have the same depth as the inputs to the layer.\n","* **Stride**(스트라이드) defines the number of pixels by which we move the filter when \"sliding\" it along the input volume. Typically this value would be 1, but values of 2 and 3 are also sometimes used.\n","* **Padding**(패딩수) refers to the addition of 0-value pixels to the edges of the input volume along the width and height dimensions. In Tensorflow you can set this to \"VALID\", which essentially does no padding or \"SAME\" which pads the input such that the output width and height are the same as the input."]},{"cell_type":"code","metadata":{"id":"iluvPEPInWKn"},"source":["# Create a random colour \"image\" of shape 10x10 with a depth of 3 (for red, green and blue)\n","dummy_input = np.random.uniform(size=[10, 10, 3])\n","fig, ax = plt.subplots(1, 1)\n","plt.imshow(dummy_input)\n","ax.grid(False)\n","print('Input shape: {}'.format(dummy_input.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZU1gD5hnsHbc"},"source":["하이퍼 파라미터를 수정하면서 결과값인 feature map의 사이즈 등이 어떻게 변화되는 지 살펴보자.\n","\n","이때, 입력 이미지의 shape은 [10, 10, 3] 이다."]},{"cell_type":"code","metadata":{"id":"OF_KxVpSpE6y"},"source":["#@title Convolutional layer parameters {run: \"auto\"}\n","filters = 5  #@param { type: \"slider\", min:0, max: 10, step: 1 }\n","kernel_size = 3 #@param { type: \"slider\", min:1, max: 10, step: 1 }\n","stride = 1 #@param { type: \"slider\", min:1, max: 3, step: 1 }\n","\n","conv_layer = tf.keras.layers.Conv2D(\n","    filters=filters,\n","    kernel_size=kernel_size,\n","    strides=stride,\n","    padding=\"valid\",\n","    input_shape=[10, 10, 3])\n","\n","# Convert the image to a tensor and add an extra batch dimension which\n","# the convolutional layer expects.\n","input_tensor = tf.convert_to_tensor(dummy_input[None, :, :, :])\n","convoluted = conv_layer(input_tensor)\n","\n","k = np.array(convoluted)\n","\n","print('The output dimension is: {}'.format(list(convoluted.shape)[1:]))\n","print('The number of parameters is: {}'.format(conv_layer.count_params()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dzh-9TL1sMCT"},"source":["여기서, 입력이미지와 출력이미지 간의 사이즈에는 관계식을 유도할수가 있다.\n","\n","\n","**(퀴즈 1) 입력이미지 시이즈, padding, stride, filter의 kernel size를 이용해서 feature map의 사이즈를 구하는 식을 구해보자. ```kernel_size``` and ```stride```, and how the output depth is related to ```filters```.**\n"]},{"cell_type":"markdown","source":["(퀴즈 2) FC 층으로 입력층(input dimension)과 출력층(output dimension)으로 하기 위해서 학습해야하는 파라미터의 개수는 어떻게 되는가?\n","\n","% print(dummy_input.flatten().shape)\n","\n","% print(k.flatten().shape)\n","\n","% print(np.dot(dummy_input.flatten().shape, k.flatten().shape))"],"metadata":{"id":"hiwzxxeyFFaE"}},{"cell_type":"markdown","source":["### [2] Fully connected layer(FC 층)\n","\n","반면에 Fully connected layer는 입력값으로 1-D로 바꿔쟈야 한다. \n","따라서 모든 feature maps를 1차원의 백터형식으로 flatten 시킨다.\n","또한, FC 에서 가중치는 완전연결계층으로 입력 노드와 출력 노드사이에 각각 연결되어 있기에 파라미터의 수는\n","입력노드 x 출력노드가 된다.\n","\n","예를 들어, 아래 이미지에서 파라미터의 개수는 $6 \\times 4 = 24$ 가 된다. 왜냐하면 입력 노트의 개수가 6개이고 출력 노드가 4개 이기 때문이다.\n","\n","![Weight Sharing](https://i.imgur.com/gcmmZz4.png)"],"metadata":{"id":"WtFOHezF6Qjx"}},{"cell_type":"markdown","metadata":{"id":"oW5FkOM2EOr3"},"source":["### (참고용 - 목적은 convolution layer을 이용해서, 다양한 feature map를 생성할 수 있다는 것을 보여줌.) \n","\n","### Optional extra reading: Building up complex filters"]},{"cell_type":"markdown","metadata":{"id":"F__JkeqMEbzv"},"source":["One of the reasons that CNNs have been so successful is their ability to build up complex filters by composing more simple filters. For example, imagine a 5 layer CNN that has been trained to detect faces. The first 4 layers are convolutional and the last layer is fully-connected and outputs the final prediction (is there a face or not). We might find that the filters in each convolution layer pick out the following features:\n","\n","1. lines (horizontal, vertical, diagonal), and colour gradients,\n","2. corners, circles and other simple shapes, and simple textures,\n","3. noses, mouths, and eyes,\n","4. whole faces.\n","\n","The neural net has learned to pick out complex objects like facial features and even whole faces! The reason for this is that each successive layer can combine the filters from the previous layer to detect more and more sophisticated features. The following diagram (adapted from [this paper](http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf)) shows some really cool examples of this kind of behaviour. The lower level features (shown above) detect noses, eyes, and mouths, in the case of faces, and wheels, doors, and windows, for cars. The higher level features are then able to detect whole faces and cars.\n","\n","![Imgur](https://i.imgur.com/653uIty.jpg)\n","\n","The diagrams on page 7 of [this classic paper](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf) show more examples of this phenomena and are definitely worth checking out!"]},{"cell_type":"markdown","metadata":{"id":"LhgDU-fVx2Jv"},"source":["### [3] (Max) Pooling 층\n","\n","Pooling 층은 공간 사이즈를 줄여주는 효과가 있습니다. 너무 많은 feature map이 있으면 학습해야하는 파라미터가 많아져 연산과정이 많아져서 속도가 많이 들고 불필요한 정보(sparsy 문제)을 줄여줄 필요가 있습니다. 이를 위해서 공간정보를 줄이면 불필요한 정보를 제거해서 오히려 모델의 성능을 개선 할 수가 있습니다. \n","\n","예를 들어, conv/pool/relu(size 20x20, 100 feature channels) 와 FC layer (10개)을 계산햇을 때,  20x20x100x10=400k parameters가 필요하다. 하지만 공간정보를 4x4 spatial size로 줄인다면 총 학습해야하는 양은 20k parameters로 줄일 수가 있다. \n","\n","pooling layer는 학습해야하는 파라미터가 존재하지 않는 장점이 있다. 차원을 축소하기 장점이 있다. 예를 들어, (2,2) 사이즈 max pooling할 경우, 가로, 세로 높이를 1/2으로 축소 할 수 있는 장점이 있다.\n","\n","pooling layer의 하이퍼 파라미터(hyper-parameter)는 아래와 같다. \n","* **Pool Size**(풀링 사이즈) defines how many values are aggregated together.\n","* **Stride**(스트라이드) defines the number of pixels by which we move the pooling filter when sliding it along the input. Typically this value would be equal to the pool size.\n","* **Padding**(패딩) refers to the addition of 0-value pixels to the edges of the input volume along the width and height dimensions. In Tensorflow you can set this to \"VALID\", which essentially does no padding or \"SAME\" which pads the input such that the output width and height are the same as the input.\n","\n","#### 퀴즈\n","아래를 max pooling으로 계산했을 때, 출력 되는 값이 어떻게 될지 출력값을 작성하시오.\n","\n","max pooling 사이즈는 (2,2)이고 stride는 2이고 padding은 없음.\n","\\begin{bmatrix}\n","  9 & 5 & 4 & 5 & 6 & 4 \\\\\n","  6 & 6 & 3 & 5 & 8 & 2 \\\\\n","  4 & 6 & 9 & 1 & 3 & 6 \\\\\n","  9 & 7 & 1 & 5 & 8 & 1 \\\\\n","  4 & 9 & 9 & 5 & 7 & 3 \\\\\n","  7 & 3 & 6 & 4 & 9 & 1 \n","\\end{bmatrix}\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"e1oIm3Nlb9zY"},"source":["#@title Answer { display-mode: \"form\" }\n","X = np.array([[9, 5, 4, 5, 6, 4],\n","              [6, 6, 3, 5, 8, 2],\n","              [4, 6, 9, 1, 3, 6],\n","              [9, 7, 1, 5, 8, 1],\n","              [4, 9, 9, 5, 7, 3],\n","              [7, 3, 6, 4, 9, 1]])\n","\n","max_pool_layer = tf.keras.layers.MaxPooling2D((2, 2), strides=2)\n","max_pool_layer(tf.convert_to_tensor(X[None, :, :, None])).numpy().squeeze()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SozPs8Ewx3tg"},"source":["### (참고용) receptive fields의 예시임.\n","\n","\n","Optional extra reading: Receptive fields"]},{"cell_type":"markdown","metadata":{"id":"XD8JDduMNh7V"},"source":["Earlier we mentioned that one reason to do pooling is to increase the sizes of the receptive fields of our features. Let's take a closer look at what we meant by this. \n","\n","The diagram below shows the effective receptive field of one output \"neuron\" in each layer of a few simple networks. What the diagram tells us is how many of the input values have an effect on each output value.\n","\n","We can see that in the first two examples, with single convolutional layers, the receptive field is simply equal to the kernel size. \n","\n","However, the next two examples are a little more interesting. Here we have drastically increased the receptive field size, without a large increase in the number of parameters, by stacking convolution and pooling layers. The interesting thing here is that by using a pooling layer we increased our receptive field size by a much smaller cost (in the number of parameters) than if we'd simply increased the kernel sizes of our convolution layers.\n","\n","You can read more about receptive fields [here](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807).\n","\n","\n","![Receptive Fields](https://i.imgur.com/TjxEsG4.png)\n"]},{"cell_type":"markdown","source":["# CNN 모델에서 중요한 요소 (시험 문제와 관련)\n","1. (실습, 이론) convoultion layer 연산 가능 \n","2. (실습, 이론)max pooling 연산 가능\n","3. CNN에서 학습해야하는 파라미터의 갯수 "],"metadata":{"id":"_GkcRTtM_T5M"}},{"cell_type":"markdown","metadata":{"id":"e4Vsrgyudd2E"},"source":["## The CIFAR10 Dataset\n","Now that we understand convolutional, max-pooling and feed-forward layers, we can combine these as building blocks to build a ConvNet classifier for images. For this practical, we will use the colour image dataset CIFAR10 (pronounced \"seefar ten\") which consists of 50,000 training images and 10,000 test images. As we did in Practical 1, we take 10,000 images from the training set to form a validation set and visualise some example images."]},{"cell_type":"code","metadata":{"id":"flWYFg3ydvMU"},"source":["cifar = tf.keras.datasets.cifar10\n","(train_images, train_labels), (test_images, test_labels) = cifar.load_data()\n","cifar_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QSzdYWpZd8RE"},"source":["# Take the last 10000 images from the training set to form a validation set\n","train_labels = train_labels.squeeze()\n","validation_images = train_images[-10000:, :, :]\n","validation_labels = train_labels[-10000:]\n","train_images = train_images[:-10000, :, :]\n","train_labels = train_labels[:-10000]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8jf1myGQP1O"},"source":["What are the shapes and data-types of train_images and train_labels?"]},{"cell_type":"code","metadata":{"id":"wzLutGn-P7mg"},"source":["print('train_images.shape = {}, data-type = {}'.format(train_images.shape, train_images.dtype))\n","print('train_labels.shape = {}, data-type = {}'.format(train_labels.shape, train_labels.dtype))\n","\n","print('validation_images.shape = {}, data-type = {}'.format(validation_images.shape, validation_images.dtype))\n","print('validation_labels.shape = {}, data-type = {}'.format(validation_labels.shape, validation_labels.dtype))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ynwzGIAneBbb"},"source":["### Visualise examples from the dataset\n","Run the cell below multiple times to see various images. (They might look a bit blurry because we've blown up the small images.)"]},{"cell_type":"code","metadata":{"id":"8nMTxCOjd9WW"},"source":["plt.figure(figsize=(10,10))\n","for i in range(25):\n","  plt.subplot(5,5,i+1)\n","  plt.xticks([])\n","  plt.yticks([])\n","  plt.grid('off')\n","\n","  img_index = np.random.randint(0, 40000)\n","  plt.imshow(train_images[img_index])\n","  plt.xlabel(cifar_labels[train_labels[img_index]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wN-XUzp-fpyp"},"source":["## A ConvNet Classifier\n","Finally, we build a simple convolutional architecture to classify the CIFAR images. We will build a mini version of the AlexNet architecture, which consists of 5 convolutional layers with max-pooling, followed by 3 fully-connected layers at the end. In order to investigate the effect each of these two layers has on the number of parameters, we'll build the model in two stages. \n","\n","First, the convolutional layers + max-pooling:"]},{"cell_type":"code","metadata":{"id":"q9zloewLws0b"},"source":["# Define the convolutinal part of the model architecture using Keras Layers.\n","model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(filters=48, kernel_size=(3, 3), activation=tf.nn.relu, input_shape=(32, 32, 3), padding='same'),\n","    tf.keras.layers.MaxPooling2D(pool_size=(3, 3)),\n","    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=tf.nn.relu, padding='same'),\n","    tf.keras.layers.MaxPooling2D(pool_size=(3, 3)),\n","    tf.keras.layers.Conv2D(filters=192, kernel_size=(3, 3), activation=tf.nn.relu, padding='same'),\n","    tf.keras.layers.Conv2D(filters=192, kernel_size=(3, 3), activation=tf.nn.relu, padding='same'),\n","    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=tf.nn.relu, padding='same'),\n","    tf.keras.layers.MaxPooling2D(pool_size=(3, 3)),\n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VHoTJ0XTgWKN"},"source":["How many parameters are there in the convolutional part of the architecture? We can easily inspect this using the model summary function in Keras:"]},{"cell_type":"code","metadata":{"id":"R4Hr3Wgtw72b"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sbJAaisIgZyX"},"source":["Now we add a fully-connected part. Note that we also add \"Dropout\" after the first fully-connected layer. Dropout is a regularization technique which randomly zeros out (\"drops\") connections between neurons, and it was one of the key innovations of the AlexNet paper in 2012."]},{"cell_type":"code","metadata":{"id":"gaHgWaNb1C0W"},"source":["model.add(tf.keras.layers.Flatten())  # Flatten \"squeezes\" a 3-D volume down into a single vector.\n","model.add(tf.keras.layers.Dense(1024, activation=tf.nn.relu))\n","model.add(tf.keras.layers.Dropout(rate=0.5))\n","model.add(tf.keras.layers.Dense(1024, activation=tf.nn.relu))\n","model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WOK9_UWx1JkL"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5uzeJaMB7DPa"},"source":["### Optional extra reading: Random initialization schemes"]},{"cell_type":"markdown","metadata":{"id":"AUBKcjfJ6gRs"},"source":["You might have wondered what values we are using for the initial values of the weights and biases in our model. The short answer is that we typically use random initialization. In this case, we have just been using the default Keras initializers for each layer, which are usually sufficient.\n","\n","The longer answer is that just using completely random numbers does not always work best in practice and that there are a number of common initialization schemes (which are available in most deep learning frameworks such as TensorFlow and Keras).\n","\n","Let's consider a few examples:\n","\n"," * When using the ReLU activation it is common to initialize the biases with small positive numbers because this encourages the ReLU activations to start off in the _on_ state, which helps to counteract the _dying ReLU problem_.\n","\n"," * The deeper neural networks become the more likely it is that gradients will either shrink to the point that they vanish, or grow to the point that they overflow (the _vanishing_ and _exploding_ gradients problems). To help combat this we can initialize our weights to have a (model-specific) appropriate scale. One method for doing this is called [Xavier or Glorot](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) initialization.\n","\n"," * The _Xavier_ initialization scheme was designed with the _traditional_ activations Sigmoid and TanH in mind and does not work as well for ReLU activations. An alternative is [He or Kaiming](https://arxiv.org/pdf/1502.01852.pdf) initialization which is a modification of Xavier initialization for ReLU activations.\n","\n"," [This blog](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization) goes into more detail on _He_ and _Xavier_ initialization. [The Keras documentation](https://keras.io/initializers/) lists a number of common schemes. "]},{"cell_type":"markdown","metadata":{"id":"nNMAg7s3W0gg"},"source":["###Visualizing the model"]},{"cell_type":"markdown","metadata":{"id":"wCL9cH8a3F1W"},"source":["Let's build a flow-diagram of the model we've constructed to see how information flows between the different layers."]},{"cell_type":"code","metadata":{"id":"dFXnOsd0W5pq"},"source":["tf.keras.utils.plot_model(model, to_file='small_lenet.png', show_shapes=True, show_layer_names=True)\n","display.display(display.Image('small_lenet.png'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFrH0OVORco2"},"source":["### Training and Validating the model\n","In the last practical we wrote out the dataset pipeline, loss function and training-loop to give you a good appreciation for how it works. This time, we use the training loop built-in to Keras. For simple, standard datasets like CIFAR, doing it this way will work fine, but it's important to know what goes on under the hood because you may need to write some or all of the steps out manually when working with more complex datasets! "]},{"cell_type":"code","metadata":{"id":"wJ6JAqUL1TDu"},"source":["batch_size = 128\n","num_epochs = 10  # The number of epochs (full passes through the data) to train for\n","\n","# Compiling the model adds a loss function, optimiser and metrics to track during training\n","model.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss=tf.keras.losses.sparse_categorical_crossentropy,\n","              metrics=['accuracy'])\n","\n","# The fit function allows you to fit the compiled model to some training data\n","model.fit(x=train_images,\n","          y=train_labels,\n","          batch_size=batch_size,\n","          epochs=num_epochs,\n","          validation_data=(validation_images, validation_labels.astype(np.float32)))\n","\n","print('Training complete')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"waS75-jNaEL1"},"source":["### Test performance\n","Finally, we evaluate how well the model does on the held-out test-set"]},{"cell_type":"code","metadata":{"id":"AVAdjH6o13tQ"},"source":["metric_values = model.evaluate(x=test_images, y=test_labels)\n","\n","print('Final TEST performance')\n","for metric_value, metric_name in zip(metric_values, model.metrics_names):\n","  print('{}: {}'.format(metric_name, metric_value))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2lsBthNB4FlL"},"source":["Note that we achieved roughly 80% training set accuracy, but our test accuracy is only around 67%. What do you think may be the reason for this?"]},{"cell_type":"markdown","metadata":{"id":"UlrSyMQpoV9f"},"source":["### Classifying examples\n","We now use our trained model to classify a sample of 25 images from the test set. We pass these 25 images to the  ```model.predict``` function, which returns a [25, 10] dimensional matrix. The entry at position $(i, j)$ of this matrix contains the probability that image $i$ belongs to class $j$. We obtain the most-likely prediction using the ```np.argmax``` function which returns the index of the maximum entry along the columns. Finally, we plot the result with the prediction and prediction probability labelled underneath the image and true label on the side. "]},{"cell_type":"code","metadata":{"id":"BjzP384wm9OW"},"source":["img_indices = np.random.randint(0, len(test_images), size=[25])\n","sample_test_images = test_images[img_indices]\n","sample_test_labels = [cifar_labels[i] for i in test_labels[img_indices].squeeze()]\n","\n","predictions = model.predict(sample_test_images)\n","max_prediction = np.argmax(predictions, axis=1)\n","prediction_probs = np.max(predictions, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ol-f9SacnySQ"},"source":["plt.figure(figsize=(10,10))\n","for i, (img, prediction, prob, true_label) in enumerate(\n","    zip(sample_test_images, max_prediction, prediction_probs, sample_test_labels)):\n","  plt.subplot(5,5,i+1)\n","  plt.xticks([])\n","  plt.yticks([])\n","  plt.grid('off')\n","\n","  plt.imshow(img)\n","  plt.xlabel('{} ({:0.3f})'.format(cifar_labels[prediction], prob))\n","  plt.ylabel('{}'.format(true_label))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0618tOoMv0Y"},"source":["### Question\n","What do you think of the model's predictions? Looking at the model's confidence (the probability assigned to the predicted class), look for examples of the following cases:\n","1. The model was correct with high confidence\n","2. The model was correct with low confidence\n","3. The model was incorrect with high confidence\n","4. The model was incorrect with low confidence\n","\n","What do you think the (relative) loss values would be in those cases? \n"]},{"cell_type":"markdown","metadata":{"id":"py0V6UwC6_kH"},"source":["### Optional extra reading: Uncertainty in deep learning"]},{"cell_type":"markdown","metadata":{"id":"lpGl8VzY6B3c"},"source":["Deep neural networks are not considered to be very good at estimating the uncertainty in their predictions. However, knowing your model's uncertainty can be very important for many applications. For example, consider a deep learning tool for diagnosing diseases, in this case, a false negative could have massive impacts on a person's life! We would really like to know how confident our model is in its prediction. This is a budding field of research, for example, see [this blog](https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html) for a nice introduction."]},{"cell_type":"markdown","metadata":{"id":"OJzCooQO66D3"},"source":["### Optional extra reading: CNN architectures"]},{"cell_type":"markdown","metadata":{"id":"n9Sjwpsm5_TD"},"source":["Deciding on the architecture for a CNN, i.e. the combination of convolution, pooling, dense, and other layers, can be tricky and often can seem arbitrary. On top of that, one also has to make decisions such as what kind of pooling, which activation functions, and what size of convolution to use, among other things. For new and old practitioners of deep learning, these choices can be overwhelming. \n","\n","However, by examining existing successful CNN architectures we can learn a lot about what works and what doesn't. (We can even apply these existing architectures to our problems since many deep learning libraries, such as TensorFlow and Keras, have them [built in](https://keras.io/applications/#available-models) and it is even possible to fine-tune pre-trained models to our specific problem using [transfer learning](https://cs231n.github.io/transfer-learning/).)\n","\n","[This article](https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5) describes many of the most successful CNN architectures in recent years, including [ResNet](https://arxiv.org/abs/1512.03385), [Inception](https://arxiv.org/pdf/1512.00567v3.pdf) and [VGG](https://arxiv.org/pdf/1409.1556.pdf). For a more detailed and technical description of these models and more see [these slides](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf). Reading through these resources should give you insights into why these architectures are successful as well as best practices and current trends for CNNs that will help you design your own architectures.\n","\n","For example, one of the practices you might pick up on is the use of 3x3 convolutions. You'll notices that older architectures such as [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) used a range of convolutions from 7x7 down to 3x3. However, newer architectures such as VGG and ResNet use 3x3 convolutions almost exclusively. In short, the reason is that stacking 3x3 convolutions gives you the same receptive field as a larger convolution but with more non-linearity. \n","\n","Here are some other questions you may want to think about while investigating these architectures:\n","\n","* Why do modern architectures use less max-pooling?\n","* What does a 1x1 convolution do?\n","* What is a residual connection?"]},{"cell_type":"markdown","metadata":{"id":"-3bIU8BErhiJ"},"source":["## Your Tasks\n","1. [**ALL**] Experiment with the network architecture, try changing the numbers, types and sizes of layers, the sizes of filters, using different padding etc. How do these decisions affect the performance of the model? In particular, try building a *fully convolutinoal* network, with no (max-)pooling layers. \n","2. [**ALL**] Add BATCH NORMALISATION ([Tensorflow documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization) and [research paper](http://proceedings.mlr.press/v37/ioffe15.pdf)) to improve the model's generalisation.\n","3. [**ADVANCED**] Read about Residual networks ([original paper](https://arxiv.org/pdf/1512.03385.pdf), ) and add **shortcut connections** to the model architecture. Try to build a simple reusable \"residual block\" as a [Keras Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model). \n","4. [**OPTIONAL**]. Visualise the filters of the convolutional layers using Matplotlib. **HINT**: You can retrieve a reference to an individual layer from the sequential Keras model by calling```model.get_layer(name)```, replacing \"name\" with the name of the layer. "]},{"cell_type":"markdown","metadata":{"id":"0e-IdtqUknDK"},"source":["##Additional Resources\n","\n","Here's some more information on ConvNets:\n","\n","* Chris Colah's blog post on [Understanding Convolutions](https://colah.github.io/posts/2014-07-Understanding-Convolutions/)\n","* [How do convolutional neural networks work?](http://brohrer.github.io/how_convolutional_neural_networks_work.html)\n","* The [CS231n course](https://cs231n.github.io/) which is a great resource that covers just about all the basics of CNNs\n","* [Building blocks of interpretability](https://distill.pub/2018/building-blocks/) (some really cool CNN feature visualisations)\n","\n"]}]}